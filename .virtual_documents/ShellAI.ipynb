import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')




#EDA
test_df=pd.read_csv("test.csv")
train_df=pd.read_csv("train.csv")
#print(test_df.info())
#test_df.describe()
train_df.describe()
train_df.info()
train_df.isnull()



# Shape and structure
print("Shape:", train_df.shape)
print("Columns:", train_df.columns.tolist())

# First few rows
display(train_df.head())

# Summary of columns
train_df.info()


# Numerical columns summary
display(train_df.describe())

# Categorical columns summary
# display(train_df.describe(include='object'))



print("Total missing values:", train_df.isnull().sum().sum())
print(train_df.isnull().sum())


import seaborn as sns
import matplotlib.pyplot as plt
sns.heatmap(train_df.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Value Heatmap")
plt.show()

print(train_df.nunique())


%pip install scipy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette('husl')
%matplotlib inline





# Load the dataset
df = pd.read_csv('train.csv')
print(f"Dataset shape: {df.shape}")
print(f"Dataset size: {df.size} values")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")


# Basic info about the dataset
print("Dataset Info:")
print(f"Number of rows: {len(df)}")
print(f"Number of columns: {len(df.columns)}")
print(f"Data types: {df.dtypes.value_counts()}")
print("\nFirst 5 rows:")
df.head()


# Check for missing values
missing_values = df.isnull().sum()
print(f"Missing values found: {missing_values.sum()}")
if missing_values.sum() > 0:
    print("Columns with missing values:")
    print(missing_values[missing_values > 0])
else:
    print("No missing values found in the dataset!")


# Basic statistical summary
print("Basic Statistical Summary:")
df.describe()

# Extract component fractions
fraction_cols = [col for col in df.columns if 'fraction' in col]
print(f"Component fraction columns: {fraction_cols}")

# Check if fractions sum to 1
fraction_sums = df[fraction_cols].sum(axis=1)
print(f"\nFraction sums statistics:")
print(f"Mean: {fraction_sums.mean():.4f}")
print(f"Std: {fraction_sums.std():.4f}")
print(f"Min: {fraction_sums.min():.4f}")
print(f"Max: {fraction_sums.max():.4f}")

# Plot fraction distributions
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for i, col in enumerate(fraction_cols):
    axes[i].hist(df[col], bins=30, alpha=0.7, edgecolor='black')
    axes[i].set_title(f'{col}')
    axes[i].set_xlabel('Fraction')
    axes[i].set_ylabel('Frequency')

# Plot sum of fractions
axes[5].hist(fraction_sums, bins=30, alpha=0.7, edgecolor='black', color='red')
axes[5].set_title('Sum of All Fractions')
axes[5].set_xlabel('Sum')
axes[5].set_ylabel('Frequency')
axes[5].axvline(x=1, color='black', linestyle='--', label='Expected sum=1')
axes[5].legend()

plt.tight_layout()
plt.show()


# Correlation between component fractions
plt.figure(figsize=(8, 6))
sns.heatmap(df[fraction_cols].corr(), annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5)
plt.title('Correlation Matrix: Component Fractions')
plt.show()

# Pairplot of fractions
g = sns.pairplot(df[fraction_cols], diag_kind='hist', corner=True)
g.fig.suptitle('Component Fractions Pairplot', y=1.02)
plt.show()


# Extract component properties for each component
components = ['Component1', 'Component2', 'Component3', 'Component4', 'Component5']
property_cols = {}

for comp in components:
    property_cols[comp] = [col for col in df.columns if comp in col and 'Property' in col]
    print(f"{comp} properties: {len(property_cols[comp])} columns")

# Statistics for each component's properties
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for i, comp in enumerate(components):
    comp_data = df[property_cols[comp]]

    # Box plot for the component
    axes[i].boxplot([comp_data[col] for col in comp_data.columns],
                   labels=[f'P{j+1}' for j in range(len(comp_data.columns))])
    axes[i].set_title(f'{comp} Properties Distribution')
    axes[i].set_ylabel('Property Value')
    axes[i].set_xlabel('Property Number')
    axes[i].grid(True, alpha=0.3)

# Remove the extra subplot
fig.delaxes(axes[5])
plt.tight_layout()
plt.show()


# Statistical summary for each property across components
property_stats = pd.DataFrame()

for i in range(1, 11):  # Properties 1-10
    prop_cols = [f'Component{j}_Property{i}' for j in range(1, 6)]

    stats_row = {
        'Property': f'Property{i}',
        'Mean': df[prop_cols].mean().mean(),
        'Std': df[prop_cols].std().mean(),
        'Min': df[prop_cols].min().min(),
        'Max': df[prop_cols].max().max(),
        'Range': df[prop_cols].max().max() - df[prop_cols].min().min()
    }
    property_stats = pd.concat([property_stats, pd.DataFrame([stats_row])], ignore_index=True)

print("Statistics across all components for each property:")
property_stats


# Extract blend properties
blend_cols = [col for col in df.columns if 'BlendProperty' in col]
print(f"Blend property columns ({len(blend_cols)}): {blend_cols}")

# Distribution of blend properties
fig, axes = plt.subplots(2, 5, figsize=(20, 10))
axes = axes.ravel()

for i, col in enumerate(blend_cols):
    axes[i].hist(df[col], bins=30, alpha=0.7, edgecolor='black')
    axes[i].set_title(f'{col}')
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Frequency')

    # Add mean line
    mean_val = df[col].mean()
    axes[i].axvline(x=mean_val, color='red', linestyle='--',
                   label=f'Mean: {mean_val:.2f}')
    axes[i].legend()

plt.tight_layout()
plt.show()

# Statistical summary for blend properties
print("\nBlend Properties Statistical Summary:")
df[blend_cols].describe()


# Correlation matrix for blend properties
plt.figure(figsize=(12, 10))
blend_corr = df[blend_cols].corr()
sns.heatmap(blend_corr, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, fmt='.2f')
plt.title('Correlation Matrix: Blend Properties')
plt.show()

# Find highly correlated pairs
corr_pairs = []
for i in range(len(blend_cols)):
    for j in range(i+1, len(blend_cols)):
        corr_val = blend_corr.iloc[i, j]
        if abs(corr_val) > 0.5:  # Threshold for high correlation
            corr_pairs.append((blend_cols[i], blend_cols[j], corr_val))

if corr_pairs:
    print("\nHighly correlated blend property pairs (|r| > 0.5):")
    for prop1, prop2, corr in corr_pairs:
        print(f"{prop1} vs {prop2}: {corr:.3f}")
else:
    print("\nNo highly correlated blend property pairs found.")


# Analyze relationship between component fractions and blend properties
print("Correlation between Component Fractions and Blend Properties:")

fraction_blend_corr = df[fraction_cols + blend_cols].corr().loc[fraction_cols, blend_cols]

plt.figure(figsize=(12, 6))
sns.heatmap(fraction_blend_corr, annot=True, cmap='coolwarm', center=0,
            linewidths=0.5, fmt='.2f')
plt.title('Correlation: Component Fractions vs Blend Properties')
plt.ylabel('Component Fractions')
plt.xlabel('Blend Properties')
plt.show()

# Find strongest correlations
strong_corrs = []
for frac in fraction_cols:
    for blend in blend_cols:
        corr_val = fraction_blend_corr.loc[frac, blend]
        if abs(corr_val) > 0.3:  # Threshold for moderate correlation
            strong_corrs.append((frac, blend, corr_val))

if strong_corrs:
    print("\nStrongest correlations between fractions and blend properties (|r| > 0.3):")
    for frac, blend, corr in sorted(strong_corrs, key=lambda x: abs(x[2]), reverse=True):
        print(f"{frac} -> {blend}: {corr:.3f}")
else:
    print("\nNo strong correlations found between fractions and blend properties.")


# Detect outliers using IQR method
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]

# Check outliers in blend properties
outlier_counts = {}
for col in blend_cols:
    outliers = detect_outliers_iqr(df, col)
    outlier_counts[col] = len(outliers)

# Plot outlier counts
plt.figure(figsize=(12, 6))
plt.bar(range(len(outlier_counts)), list(outlier_counts.values()))
plt.xticks(range(len(outlier_counts)), list(outlier_counts.keys()), rotation=45)
plt.title('Number of Outliers per Blend Property (IQR Method)')
plt.ylabel('Number of Outliers')
plt.tight_layout()
plt.show()

print("Outlier summary:")
for prop, count in outlier_counts.items():
    percentage = (count / len(df)) * 100
    print(f"{prop}: {count} outliers ({percentage:.1f}%)")


# Data quality summary
print("=== DATA QUALITY ASSESSMENT ===")
print(f"\n1. DATASET SIZE:")
print(f"   - Rows: {len(df):,}")
print(f"   - Columns: {len(df.columns)}")
print(f"   - Total values: {df.size:,}")

print(f"\n2. MISSING VALUES:")
missing_count = df.isnull().sum().sum()
print(f"   - Total missing: {missing_count}")
print(f"   - Percentage: {(missing_count/df.size)*100:.2f}%")

print(f"\n3. DATA TYPES:")
for dtype, count in df.dtypes.value_counts().items():
    print(f"   - {dtype}: {count} columns")

print(f"\n4. COMPONENT FRACTIONS:")
fraction_sums = df[fraction_cols].sum(axis=1)
valid_fractions = ((fraction_sums >= 0.99) & (fraction_sums <= 1.01)).sum()
print(f"   - Valid fraction sums (0.99-1.01): {valid_fractions}/{len(df)} ({(valid_fractions/len(df))*100:.1f}%)")

print(f"\n5. OUTLIERS (using IQR method):")
total_outliers = sum(outlier_counts.values())
print(f"   - Total outlier instances: {total_outliers}")
print(f"   - Percentage of data points: {(total_outliers/(len(df)*len(blend_cols)))*100:.1f}%")

print(f"\n6. VALUE RANGES:")
print(f"   - Component fractions: {df[fraction_cols].min().min():.3f} to {df[fraction_cols].max().max():.3f}")
print(f"   - Blend properties: {df[blend_cols].min().min():.3f} to {df[blend_cols].max().max():.3f}")

print(f"\n=== SUMMARY ===")
print(f"The dataset appears to be of high quality with:")
print(f"- No missing values")
print(f"- Consistent data types (all numeric)")
print(f"- Valid component fractions that sum to approximately 1")
print(f"- Reasonable value ranges for all features")
print(f"- Manageable number of outliers")














from sklearn.model_selection import train_test_split

X = df.drop('BlendProperty1', axis=1)
y = df['BlendProperty1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)





import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('train.csv')

X = df.drop('BlendProperty1', axis=1)
y = df['BlendProperty1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)








from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)





from sklearn.model_selection import train_test_split

X = df.drop('BlendProperty1', axis=1)
y = df['BlendProperty1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)





import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

df = pd.read_csv('train.csv')

X = df.drop('BlendProperty1', axis=1)
y = df['BlendProperty1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)








from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")
print(f"R-squared: {r2:.4f}")


# from sklearn.model_selection import GridSearchCV

# param_grid = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [None, 10, 20],
#     'min_samples_split': [2, 5, 10]
# }

# grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')
# grid.fit(X_train, y_train)

# print("Best parameters:", grid.best_params_)








y_pred = model.predict(X_test)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual BlendProperty1')
plt.ylabel('Predicted BlendProperty1')
plt.title('Actual vs. Predicted BlendProperty1')
plt.grid(True, alpha=0.3)
plt.show()











from sklearn.model_selection import train_test_split

X = df.drop('BlendProperty1', axis=1)
y = df['BlendProperty1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)








from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)


# from sklearn.model_selection import GridSearchCV

# param_grid = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [None, 10, 20],
#     'min_samples_split': [2, 5, 10]
# }

# grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')
# grid.fit(X_train, y_train)

# print("Best parameters:", grid.best_params_)








from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")
print(f"R-squared: {r2:.4f}")


import pandas as pd
from sklearn.model_selection import train_test_split

# Load the data
df = pd.read_csv('train.csv')

# Separate features and multiple target variables
X = df.drop(columns=[f'BlendProperty{i}' for i in range(1, 11)])
y = df[[f'BlendProperty{i}' for i in range(1, 11)]]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor

# Define the base regressor
base_model = RandomForestRegressor(random_state=42)

# Wrap with MultiOutputRegressor
model = MultiOutputRegressor(base_model)

# Train the model
model.fit(X_train, y_train)


from sklearn.metrics import mean_absolute_percentage_error

# Make predictions
y_pred = model.predict(X_test)

# Evaluate: Mean Absolute Percentage Error for each target
for i in range(10):
    mape = mean_absolute_percentage_error(y_test.iloc[:, i], y_pred[:, i])
    print(f"BlendProperty{i+1} MAPE: {mape:.4f}")


import seaborn as sns
sns.histplot(y['BlendProperty1'], kde=True)


correlations = df.corr()['BlendProperty1'].sort_values(ascending=False)
print(correlations)


import matplotlib.pyplot as plt
import seaborn as sns

# Compute correlation matrix
corr_matrix = df.corr()

# Set the size of the heatmap
plt.figure(figsize=(14, 12))

# Create the heatmap
sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.2f', linewidths=0.5)

# Plot formatting
plt.title('Correlation Matrix Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()


# Extract only BlendProperty columns
target_cols = [f'BlendProperty{i}' for i in range(1, 11)]
feature_cols = [col for col in df.columns if col not in target_cols]

# Correlation between features and targets
focused_corr = df[feature_cols + target_cols].corr().loc[feature_cols, target_cols]

# Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(focused_corr, cmap='viridis', annot=True, fmt='.2f')
plt.title('Feature vs BlendProperty Correlations', fontsize=14)
plt.tight_layout()
plt.show()


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Compute correlations with BlendProperty1
blend1_corr = df.corr()['BlendProperty1'].drop('BlendProperty1')

# Get top 10 absolute correlations (positive or negative)
top_corr = blend1_corr.reindex(blend1_corr.abs().sort_values(ascending=False).index).head(10)

# Print values
print("Top 10 correlations with BlendProperty1:\n")
print(top_corr)

# Plot barplot
plt.figure(figsize=(10, 6))
sns.barplot(x=top_corr.values, y=top_corr.index, palette='coolwarm')
plt.xlabel('Correlation with BlendProperty1')
plt.title('Top Correlated Features with BlendProperty1')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()


import pandas as pd
import plotly.express as px

# Compute correlations
blend1_corr = df.corr()['BlendProperty1'].drop('BlendProperty1')

# Get top 10 absolute correlations
top_corr = blend1_corr.reindex(blend1_corr.abs().sort_values(ascending=False).index).head(10)

# Convert to DataFrame for Plotly
corr_df = top_corr.reset_index()
corr_df.columns = ['Feature', 'Correlation']

# Plot interactive bar chart
fig = px.bar(
    corr_df,
    x='Correlation',
    y='Feature',
    orientation='h',
    title='Top 10 Correlated Features with BlendProperty1',
    color='Correlation',
    color_continuous_scale='RdBu',
    text='Correlation'
)

fig.update_layout(
    yaxis=dict(autorange="reversed"),
    plot_bgcolor='rgba(0,0,0,0)',
    xaxis_gridcolor='lightgray',
    yaxis_gridcolor='lightgray'
)

fig.show()


from sklearn.decomposition import PCA

# Apply PCA to targets (you can choose how many components to retain)
pca = PCA(n_components=5)  # or try 0.95 to retain 95% variance
y_train_pca = pca.fit_transform(y_train)
y_test_pca = pca.transform(y_test)

print("Explained variance ratio by each component:")
print(pca.explained_variance_ratio_)


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_percentage_error
import matplotlib.pyplot as plt

# ---------- Step 1: Load and Split ----------
df = pd.read_csv('train.csv')

target_cols = [f'BlendProperty{i}' for i in range(1, 11)]
X = df.drop(columns=target_cols)
y = df[target_cols]

# Standardize features (optional but recommended for models like PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ---------- Step 2: Apply PCA on Targets ----------
pca = PCA(n_components=5)  # Based on explained variance output
y_train_pca = pca.fit_transform(y_train)
y_test_pca = pca.transform(y_test)

# ---------- Step 3: Train Model on PCA Components ----------
base_model = RandomForestRegressor(random_state=42)
model = MultiOutputRegressor(base_model)
model.fit(X_train, y_train_pca)

# ---------- Step 4: Predict and Inverse Transform ----------
y_pred_pca = model.predict(X_test)
y_pred_original = pca.inverse_transform(y_pred_pca)

# ---------- Step 5: Evaluate Using MAPE ----------
print("MAPE per BlendProperty (after inverse PCA):")
for i in range(10):
    mape = mean_absolute_percentage_error(y_test.iloc[:, i], y_pred_original[:, i])
    print(f"BlendProperty{i+1} MAPE: {mape:.4f}")


# Train a dedicated model just for BlendProperty1
model_bp1 = RandomForestRegressor(random_state=42)
model_bp1.fit(X_train, y_train['BlendProperty1'])
y_pred_bp1 = model_bp1.predict(X_test)

# MAPE
from sklearn.metrics import mean_absolute_percentage_error
print("Standalone BlendProperty1 MAPE:", mean_absolute_percentage_error(y_test['BlendProperty1'], y_pred_bp1))



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_percentage_error

# Load data
df = pd.read_csv('train.csv')
target_cols = [f'BlendProperty{i}' for i in range(1, 11)]

# Separate features and targets
X = df.drop(columns=target_cols)
y = df[target_cols]

# Standardize X
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# --------------------------------------------
# STAGE 1: Predict correlated properties (2, 4, 6, 8)
# --------------------------------------------
stage1_targets = ['BlendProperty2', 'BlendProperty4', 'BlendProperty6', 'BlendProperty8']
stage1_preds_train = pd.DataFrame(index=y_train.index)
stage1_preds_test = pd.DataFrame(index=y_test.index)

for col in stage1_targets:
    model = XGBRegressor(
        loss="reg:absoluteerror",
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        tree_method='hist',
        device='cuda',
        random_state=42
    )
    model.fit(X_train, y_train[col])
    stage1_preds_train[col] = model.predict(X_train)
    stage1_preds_test[col] = model.predict(X_test)

# --------------------------------------------
# STAGE 2: Predict BlendProperty1 using original X + stage1 predictions
# --------------------------------------------
# Augment original features with stage1 predictions
X_train_stacked = np.hstack([X_train, stage1_preds_train.values])
X_test_stacked = np.hstack([X_test, stage1_preds_test.values])

# Train final model for BlendProperty1
bp1_model = XGBRegressor(
    #objective='reg:pseudohubererror',
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    tree_method='hist',
    device='cuda',
    random_state=42
)
bp1_model.fit(X_train_stacked, y_train['BlendProperty1'])

# Predict & Evaluate
bp1_pred = bp1_model.predict(X_test_stacked)
bp1_mape = mean_absolute_percentage_error(y_test['BlendProperty1'], bp1_pred)

print(f"ðŸ“Š BlendProperty1 MAPE (with stacked features): {bp1_mape:.4f}")


import matplotlib.pyplot as plt
import seaborn as sns

# Compute residuals
residuals = y_test['BlendProperty1'] - bp1_pred

# Scatter plot: predicted vs residuals
plt.figure(figsize=(8, 5))
sns.scatterplot(x=bp1_pred, y=residuals, alpha=0.6, edgecolor=None)
plt.axhline(0, color='red', linestyle='--', linewidth=1)
plt.title("Residuals vs Predicted Values for BlendProperty1")
plt.xlabel("Predicted BlendProperty1")
plt.ylabel("Residual (Actual - Predicted)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


sns.histplot(residuals, bins=40, kde=True, color='salmon')
plt.title("Distribution of Residuals for BlendProperty1")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Frequency")
plt.grid(True, alpha=0.3)
plt.show()


import pandas as pd
import numpy as np

# Assuming df is your original dataframe
component_cols = [col for col in df.columns if 'Component' in col and 'fraction' in col]

# Make a copy of input features only (no targets yet)
X_components = df[component_cols].copy()

# -------------------------
# 1. Add Pairwise Multiplications
# -------------------------
from itertools import combinations

for col1, col2 in combinations(component_cols, 2):
    new_col = f'{col1}_x_{col2}'
    X_components[new_col] = df[col1] * df[col2]

# -------------------------
# 2. Add Squared Terms
# -------------------------
for col in component_cols:
    X_components[f'{col}_squared'] = df[col] ** 2

# -------------------------
# 3. (Optional) Add Safe Ratios
# -------------------------
for col1, col2 in combinations(component_cols, 2):
    new_col = f'{col1}_div_{col2}'
    # Avoid division by 0
    X_components[new_col] = df[col1] / (df[col2] + 1e-6)

# -------------------------
# 4. Combine with your blend targets
# -------------------------
blend_targets = [f'BlendProperty{i}' for i in range(1, 11)]
df_augmented = pd.concat([X_components, df[blend_targets]], axis=1)
