{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be8f449-bffc-4442-8916-f5fcd820ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer,QuantileTransformer,RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from optuna.integration import CatBoostPruningCallback  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6c61a0-2817-493d-ba85-27023c2ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"/\", \"Volumes\", \"Extreme SSD\", \"ShellAi\")\n",
    "train_path = os.path.join(base_path, \"train.csv\")\n",
    "test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f52e2ac-8327-44c8-bc3e-af91dcb4c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating  weighted columns\n",
    "base_features = [col for col in train_df.columns if col not in [f\"BlendProperty{i}\" for i in range(1, 11)]]\n",
    "targets = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "weighted_data = {}\n",
    "for i in range(1, 6):\n",
    "    for j in range(1, 11):\n",
    "        prop_col = f\"Component{i}_Property{j}\"\n",
    "        frac_col = f\"Component{i}_fraction\"\n",
    "        weighted_col = f\"Weighted_Component{i}_Property{j}\"\n",
    "        weighted_data[weighted_col] = train_df[prop_col] * train_df[frac_col]\n",
    "\n",
    "# Combine\n",
    "data_with_weighted = pd.concat([train_df[base_features], pd.DataFrame(weighted_data)], axis=1)\n",
    "data_with_weighted = data_with_weighted.copy()  # De-fragmented copy\n",
    "# Add quadratic terms for each component fraction\n",
    "for i in range(1, 6):\n",
    "    frac_col = f\"Component{i}_fraction\"\n",
    "    quad_col = f\"Quadratic_{frac_col}\"\n",
    "    data_with_weighted[quad_col] = train_df[frac_col] ** 2\n",
    "\n",
    "# Add pairwise products of fractions (all unique pairs to avoid redundancy)\n",
    "import itertools\n",
    "fractions = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
    "for pair in itertools.combinations(fractions, 2):\n",
    "    col_name = f\"Pairwise_{pair[0]}_x_{pair[1]}\"\n",
    "    data_with_weighted[col_name] = train_df[pair[0]] * train_df[pair[1]]\n",
    "\n",
    "# Proceed with scaling (your existing code)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data_with_weighted)\n",
    "scaled_df_features = pd.DataFrame(scaled_features, columns=data_with_weighted.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a99a28c0-0572-432d-b4f8-0777edf05e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data_with_weighted)\n",
    "scaled_df_features = pd.DataFrame(scaled_features, columns=data_with_weighted.columns)\n",
    "\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transformed_targets_df = pd.DataFrame(pt.fit_transform(train_df[targets]), columns=targets, index=train_df.index)\n",
    "\n",
    "# Initial feature selection using Random Forest feature importance\n",
    "top_features = {}\n",
    "for target in targets:\n",
    "    X = scaled_df_features\n",
    "    y = transformed_targets_df[target]\n",
    "    rf_initial = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_initial.fit(X, y)\n",
    "    feature_importance = pd.Series(rf_initial.feature_importances_, index=scaled_df_features.columns).sort_values(ascending=False)\n",
    "    top_features[target] = feature_importance.head(20).index.tolist()\n",
    "\n",
    "# GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20,]\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b56f68-10f4-4b7d-b37f-4494d339116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Target: BlendProperty1\n",
      "Best params: {'max_depth': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "for target in targets:\n",
    "    print(f\"\\nüîπ Target: {target}\")\n",
    "\n",
    "    X = scaled_df_features[top_features[target]]\n",
    "    y = transformed_targets_df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5,\n",
    "                               scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best params: {grid_search.best_params_}\")\n",
    "\n",
    "    # CV MAPE on transformed targets\n",
    "    cv_mape = -cross_val_score(best_model, X_train, y_train, cv=5,\n",
    "                               scoring='neg_mean_absolute_percentage_error').mean() * 100\n",
    "    print(f\"Cross-validated MAPE (transformed): {cv_mape:.2f}%\")\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform for original scale MAPE\n",
    "    y_test_full = pd.DataFrame(np.column_stack([y_test] + [np.zeros(len(y_test))]*(len(targets)-1)),\n",
    "                               columns=targets, index=X_test.index)\n",
    "    y_pred_full = pd.DataFrame(np.column_stack([y_pred] + [np.zeros(len(y_pred))]*(len(targets)-1)),\n",
    "                               columns=targets, index=X_test.index)\n",
    "\n",
    "    y_test_inv = pt.inverse_transform(y_test_full)[targets.index(target)]\n",
    "    y_pred_inv = pt.inverse_transform(y_pred_full)[targets.index(target)]\n",
    "\n",
    "    # Safe MAPE\n",
    "    mask = y_test_inv != 0\n",
    "    test_mape = np.mean(np.abs((y_test_inv[mask] - y_pred_inv[mask]) / y_test_inv[mask]) * 100) if mask.any() else 0.0\n",
    "    print(f\"Test MAPE (original scale): {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff71d36-948f-46a1-81d3-167568491e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "best_models = {}  # ‚Üê define this before the loop\n",
    "\n",
    "# Inside the for-loop after training\n",
    " # ‚Üê define this before the loop\n",
    "\n",
    "# Inside the for-loop after training\n",
    "\n",
    "\n",
    "# Loop through each target\n",
    "for target in targets:\n",
    "    print(f\"\\nüîπ Optimizing Target: {target}\")\n",
    "\n",
    "    X = scaled_df_features[top_features[target]]\n",
    "    y = transformed_targets_df[target]\n",
    "\n",
    "    # Split for Optuna\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define objective function\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10.0),\n",
    "            \"loss_function\": \"MAPE\",\n",
    "            \"verbose\": 0\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_valid)\n",
    "        return mean_absolute_percentage_error(y_valid, preds)\n",
    "\n",
    "    # Run Optuna\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"‚úÖ Best params for {target}: {best_params}\")\n",
    "\n",
    "    # Train on full training data\n",
    "    best_model = CatBoostRegressor(**best_params)\n",
    "    best_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "    # Predict on test\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform for original scale MAPE\n",
    "    y_test_full = pd.DataFrame(np.column_stack([y_test] + [np.zeros(len(y_test))]*(len(targets)-1)),\n",
    "                               columns=targets, index=X_test.index)\n",
    "    y_pred_full = pd.DataFrame(np.column_stack([y_pred] + [np.zeros(len(y_pred))]*(len(targets)-1)),\n",
    "                               columns=targets, index=X_test.index)\n",
    "\n",
    "    y_test_inv = pt.inverse_transform(y_test_full)[targets.index(target)]\n",
    "    y_pred_inv = pt.inverse_transform(y_pred_full)[targets.index(target)]\n",
    "\n",
    "    # Safe MAPE\n",
    "    mask = y_test_inv != 0\n",
    "    test_mape = np.mean(np.abs((y_test_inv[mask] - y_pred_inv[mask]) / y_test_inv[mask]) * 100) if mask.any() else 0.0\n",
    "    best_models[target] = best_model\n",
    "    print(f\"üìâ Test MAPE (original scale): {test_mape:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355827e-87a7-4190-97ab-32e616c657d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# STEP 1: Create weighted features for test_df\n",
    "weighted_data_test = {}\n",
    "for i in range(1, 6):\n",
    "    for j in range(1, 11):\n",
    "        prop_col = f\"Component{i}_Property{j}\"\n",
    "        frac_col = f\"Component{i}_fraction\"\n",
    "        weighted_col = f\"Weighted_Component{i}_Property{j}\"\n",
    "        weighted_data_test[weighted_col] = test_df[prop_col] * test_df[frac_col]\n",
    "\n",
    "# Add quadratic terms for each component fraction (test data)\n",
    "for i in range(1, 6):\n",
    "    frac_col = f\"Component{i}_fraction\"\n",
    "    quad_col = f\"Quadratic_{frac_col}\"\n",
    "    data_with_weighted_test[quad_col] = test_df[frac_col] ** 2\n",
    "\n",
    "# Add pairwise products of fractions (test data)\n",
    "fractions = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
    "for pair in itertools.combinations(fractions, 2):\n",
    "    col_name = f\"Pairwise_{pair[0]}_x_{pair[1]}\"\n",
    "    data_with_weighted_test[col_name] = test_df[pair[0]] * test_df[pair[1]]\n",
    "\n",
    "# Proceed with scaling (your existing code, using the scaler fitted on training data)\n",
    "scaled_test_features = scaler.transform(data_with_weighted_test)\n",
    "scaled_df_test = pd.DataFrame(scaled_test_features, columns=data_with_weighted_test.columns)\n",
    "\n",
    "\n",
    "# STEP 2: Combine with base features\n",
    "base_features = [col for col in test_df.columns if col not in [f\"BlendProperty{i}\" for i in range(1, 11)]]\n",
    "data_with_weighted_test = pd.concat([test_df[base_features], pd.DataFrame(weighted_data_test)], axis=1)\n",
    "data_with_weighted_test = data_with_weighted_test.copy()\n",
    "if 'ID' in data_with_weighted_test.columns:\n",
    "    data_with_weighted_test = data_with_weighted_test.drop(columns=['ID'])\n",
    "\n",
    "\n",
    "# STEP 3: Scale test data using training-time scaler (RobustScaler)\n",
    "scaled_test_features = scaler.transform(data_with_weighted_test)\n",
    "scaled_df_test = pd.DataFrame(scaled_test_features, columns=data_with_weighted_test.columns)\n",
    "\n",
    "# STEP 4: Predict for each target using best Optuna model + selected features\n",
    "all_preds_transformed = []\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"üîπ Predicting for: {target}\")\n",
    "    \n",
    "    model = best_models[target]\n",
    "    selected_features = top_features[target]\n",
    "    X_test_target = scaled_df_test[selected_features]\n",
    "    \n",
    "    y_pred_transformed = model.predict(X_test_target)\n",
    "    all_preds_transformed.append(y_pred_transformed)\n",
    "\n",
    "# STEP 5: Inverse transform the predictions to original scale\n",
    "all_preds_transformed = np.column_stack(all_preds_transformed)  # Shape: (n_samples, 10)\n",
    "all_preds_original = pt.inverse_transform(all_preds_transformed)\n",
    "\n",
    "# STEP 6: Create submission DataFrame\n",
    "submission = pd.DataFrame(all_preds_original, columns=targets, index=test_df.index)\n",
    "\n",
    "# Include 'ID' column if present\n",
    "if 'ID' in test_df.columns:\n",
    "    submission.insert(0, 'ID', test_df['ID'].values)\n",
    "\n",
    "# STEP 7: Save to CSV\n",
    "submission.to_csv(\"submission5.csv\", index=False)\n",
    "print(\"‚úÖ submission5.csv saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (calorie-env)",
   "language": "python",
   "name": "calorie-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
