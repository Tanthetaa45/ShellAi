{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SETUP AND DATA LOADING ---\n",
    "TARGET_TO_SOLVE = 'BlendProperty4'\n",
    "MODEL_SAVE_DIR = f\"saved_models_{TARGET_TO_SOLVE}\"\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# (The outlier removal and basic feature engineering code remains here)\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = df[column].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "for column in data.columns: data = remove_outliers(data, column)\n",
    "\n",
    "print(\"Creating base features...\")\n",
    "statistical_features = pd.DataFrame(index=data.index)\n",
    "for j in range(1, 11):\n",
    "    prop_cols = [f\"Component{i}_Property{j}\" for i in range(1, 6)]\n",
    "    statistical_features[f'Prop{j}_mean'] = data[prop_cols].mean(axis=1)\n",
    "    statistical_features[f'Prop{j}_std'] = data[prop_cols].std(axis=1)\n",
    "    \n",
    "weighted_data = {}\n",
    "base_features_cols = [col for col in data.columns if 'Property' in col or 'fraction' in col]\n",
    "for i in range(1, 6):\n",
    "    for j in range(1, 11):\n",
    "        prop_col = f\"Component{i}_Property{j}\"\n",
    "        frac_col = f\"Component{i}_fraction\"\n",
    "        weighted_col = f\"Weighted_Component{i}_Property{j}\"\n",
    "        weighted_data[weighted_col] = data[prop_col] * data[frac_col]\n",
    "base_features = pd.concat([data[base_features_cols], pd.DataFrame(weighted_data), statistical_features], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2782be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. ADVANCED FEATURE ENGINEERING ---\n",
    "print(f\"Creating advanced interaction features for {TARGET_TO_SOLVE}...\")\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(base_features)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=base_features.columns)\n",
    "\n",
    "y_target = data[[TARGET_TO_SOLVE]]\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "y_transformed = pt.fit_transform(y_target)\n",
    "\n",
    "rf_temp = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rf_temp.fit(X_scaled_df, y_transformed.ravel())\n",
    "importances = pd.Series(rf_temp.feature_importances_, index=X_scaled_df.columns)\n",
    "top_10_base_features = importances.nlargest(10).index\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features = poly.fit_transform(X_scaled_df[top_10_base_features])\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(top_10_base_features))\n",
    "\n",
    "X_advanced = pd.concat([X_scaled_df, poly_df], axis=1)\n",
    "X_advanced = X_advanced.loc[:,~X_advanced.columns.duplicated()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. ADVANCED FEATURE SELECTION (RFECV) ---\n",
    "print(\"Finding optimal number of features with RFECV...\")\n",
    "estimator_rfe = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "selector = RFECV(estimator_rfe, step=1, cv=KFold(3), scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "selector.fit(X_advanced, y_transformed.ravel())\n",
    "print(f\"RFECV selected {selector.n_features_} features as optimal.\")\n",
    "X_final = selector.transform(X_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df16cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. TRAIN / TEST SPLIT (used for all models) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_transformed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base features...\n",
      "Creating advanced interaction features for BlendProperty4...\n",
      "Finding optimal number of features with RFECV...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44852\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44800\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44545\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 178\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44290\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 177\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44035\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43780\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 175\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43525\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43270\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43239\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 172\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 42984\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 171\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 42729\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 170\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 42474\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 169\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 42219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 168\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41964\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41709\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41454\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 165\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 41199\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40944\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 163\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40689\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40434\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40179\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 160\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39924\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 159\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39872\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 158\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39617\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 157\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39362\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 156\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39107\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38852\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38597\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 153\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38342\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 152\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38087\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 151\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37832\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 150\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37577\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 149\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37322\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 148\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37067\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36812\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 146\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36557\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36302\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 144\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36047\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 143\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35792\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35537\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 141\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35282\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 140\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 35027\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 139\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34772\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 138\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34517\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 137\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34262\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 136\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34007\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 135\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33752\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 134\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33497\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33242\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32987\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 131\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32732\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 130\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32477\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32222\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31967\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 127\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31712\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 126\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31457\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 125\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31202\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 124\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30947\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30692\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 122\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30437\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004834 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30182\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29929\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29674\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29419\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29164\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29112\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 115\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28857\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 114\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28602\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 113\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28347\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 112\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28092\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 111\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27837\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 110\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27585\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27330\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 108\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 27075\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 107\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26820\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26565\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 105\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26310\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 104\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26055\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 103\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25800\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 102\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25545\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 101\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25290\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25035\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24780\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24525\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 97\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24270\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 96\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24015\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 95\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23760\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23505\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23250\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22995\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22740\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 90\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22485\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 89\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22230\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21975\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 87\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21720\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21465\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21210\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 84\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20955\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20700\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20445\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20190\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19935\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19680\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 78\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19426\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19171\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 76\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18916\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 75\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18661\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18406\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18151\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 72\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17896\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 71\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17641\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17386\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17131\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 68\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16876\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16621\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 66\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16366\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 65\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16111\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15856\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15601\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 62\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15551\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15296\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 60\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15041\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14786\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14531\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14276\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14021\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13766\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13511\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13256\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 52\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13001\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 51\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12746\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12236\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 47\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11727\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 46\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11472\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 45\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11217\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10962\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 43\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10707\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10452\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10197\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9942\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9687\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9432\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9177\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8667\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8412\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8157\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7902\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 31\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7647\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7392\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7137\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6882\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6627\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6372\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6117\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5862\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 23\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5607\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5352\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5097\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4842\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4587\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4332\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4078\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3823\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3568\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3313\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3058\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2803\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2548\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2293\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2038\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1783\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1528\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1273\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1018\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 763\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 509\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 254\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 254\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "RFECV selected 1 features as optimal.\n",
      "\n",
      "--- Training Model 1: Ridge Regression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-19 01:35:45,177] A new study created in memory with name: no-name-9e83b9a8-8be3-45f2-a534-4402868a5e3a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge Alpha: 1.0\n",
      "Final Test MAPE for Ridge: 29.28%\n",
      "\n",
      "--- Training Model 2: LightGBM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-19 01:35:46,497] Trial 0 finished with value: 0.044838768594467344 and parameters: {'learning_rate': 0.02739216024356818, 'num_leaves': 242, 'max_depth': 5, 'subsample': 0.6854040338161675, 'colsample_bytree': 0.8244523427879662}. Best is trial 0 with value: 0.044838768594467344.\n",
      "[I 2025-07-19 01:35:46,587] Trial 1 finished with value: 0.31252588004000914 and parameters: {'learning_rate': 0.0020563194321981974, 'num_leaves': 27, 'max_depth': 8, 'subsample': 0.6429106461839705, 'colsample_bytree': 0.5658400280213931}. Best is trial 0 with value: 0.044838768594467344.\n",
      "[I 2025-07-19 01:35:46,921] Trial 3 finished with value: 0.041263108552252674 and parameters: {'learning_rate': 0.035580614918240445, 'num_leaves': 181, 'max_depth': 12, 'subsample': 0.7057963060651161, 'colsample_bytree': 0.87271335363851}. Best is trial 3 with value: 0.041263108552252674.\n",
      "[I 2025-07-19 01:35:47,214] Trial 2 finished with value: 0.20802160450919788 and parameters: {'learning_rate': 0.0028236129253160105, 'num_leaves': 177, 'max_depth': 10, 'subsample': 0.9773235957260529, 'colsample_bytree': 0.41975875080905994}. Best is trial 3 with value: 0.041263108552252674.\n",
      "[I 2025-07-19 01:35:47,803] Trial 5 finished with value: 0.04196740333551854 and parameters: {'learning_rate': 0.04202317682245437, 'num_leaves': 83, 'max_depth': 5, 'subsample': 0.6046346209914732, 'colsample_bytree': 0.6716630646995525}. Best is trial 3 with value: 0.041263108552252674.\n",
      "[I 2025-07-19 01:35:47,941] Trial 6 finished with value: 0.04208428209525673 and parameters: {'learning_rate': 0.08125882011826634, 'num_leaves': 43, 'max_depth': 4, 'subsample': 0.9410757677427752, 'colsample_bytree': 0.689431441924151}. Best is trial 3 with value: 0.041263108552252674.\n",
      "[I 2025-07-19 01:35:48,354] Trial 7 finished with value: 0.10082014124111477 and parameters: {'learning_rate': 0.004515605454739847, 'num_leaves': 236, 'max_depth': 5, 'subsample': 0.903014542836389, 'colsample_bytree': 0.5342858148999345}. Best is trial 3 with value: 0.041263108552252674.\n",
      "[I 2025-07-19 01:35:48,473] Trial 4 finished with value: 0.04118840038283668 and parameters: {'learning_rate': 0.04335702146433827, 'num_leaves': 68, 'max_depth': 9, 'subsample': 0.8238242844119331, 'colsample_bytree': 0.687694044847882}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:48,944] Trial 9 finished with value: 0.05186056188938151 and parameters: {'learning_rate': 0.006320116940637986, 'num_leaves': 172, 'max_depth': 5, 'subsample': 0.9300559641450393, 'colsample_bytree': 0.7768881415307234}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:49,099] Trial 8 finished with value: 0.4838175327720748 and parameters: {'learning_rate': 0.0012679125269518202, 'num_leaves': 115, 'max_depth': 12, 'subsample': 0.7192010411312342, 'colsample_bytree': 0.9688554413566848}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:49,335] Trial 11 finished with value: 0.04174595379353549 and parameters: {'learning_rate': 0.02936031304155132, 'num_leaves': 175, 'max_depth': 10, 'subsample': 0.6095944680540687, 'colsample_bytree': 0.4263575628243805}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:49,528] Trial 10 finished with value: 0.04224130766384913 and parameters: {'learning_rate': 0.08706159786485869, 'num_leaves': 162, 'max_depth': 5, 'subsample': 0.8459510547208641, 'colsample_bytree': 0.7932401456875433}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:50,439] Trial 12 finished with value: 0.04127042217917929 and parameters: {'learning_rate': 0.017060849434837357, 'num_leaves': 58, 'max_depth': 9, 'subsample': 0.7365767306617288, 'colsample_bytree': 0.9201224676286605}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:50,534] Trial 13 finished with value: 0.043845298731472 and parameters: {'learning_rate': 0.00957362959245387, 'num_leaves': 297, 'max_depth': 8, 'subsample': 0.83174223243821, 'colsample_bytree': 0.5730841175746789}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:51,152] Trial 14 finished with value: 0.042073219950311484 and parameters: {'learning_rate': 0.013643365003001952, 'num_leaves': 293, 'max_depth': 12, 'subsample': 0.7956570858916794, 'colsample_bytree': 0.9139690571335747}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:51,519] Trial 15 finished with value: 0.041607778103429666 and parameters: {'learning_rate': 0.012795400664547636, 'num_leaves': 299, 'max_depth': 12, 'subsample': 0.7807779708000215, 'colsample_bytree': 0.9494826422931268}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:52,168] Trial 19 finished with value: 0.04139357645953038 and parameters: {'learning_rate': 0.051589669591728775, 'num_leaves': 112, 'max_depth': 10, 'subsample': 0.7499059581654437, 'colsample_bytree': 0.8480302522930485}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:52,240] Trial 16 finished with value: 0.041473606498703526 and parameters: {'learning_rate': 0.013250785058446691, 'num_leaves': 111, 'max_depth': 12, 'subsample': 0.7884456783874576, 'colsample_bytree': 0.8769164558552819}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:52,383] Trial 17 finished with value: 0.04125138931288321 and parameters: {'learning_rate': 0.048633030841938665, 'num_leaves': 116, 'max_depth': 12, 'subsample': 0.8070987892345644, 'colsample_bytree': 0.8750089470939767}. Best is trial 4 with value: 0.04118840038283668.\n",
      "[I 2025-07-19 01:35:52,965] Trial 18 finished with value: 0.041015355057458125 and parameters: {'learning_rate': 0.04641788831220958, 'num_leaves': 116, 'max_depth': 11, 'subsample': 0.7732171710988691, 'colsample_bytree': 0.8586909418857228}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:53,455] Trial 22 finished with value: 0.04151042411332295 and parameters: {'learning_rate': 0.06034648445532592, 'num_leaves': 86, 'max_depth': 7, 'subsample': 0.8599498140199253, 'colsample_bytree': 0.7410350380046669}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:53,551] Trial 20 finished with value: 0.04165836913349959 and parameters: {'learning_rate': 0.02199637988108882, 'num_leaves': 129, 'max_depth': 7, 'subsample': 0.8736895907311292, 'colsample_bytree': 0.7415730477563293}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:53,690] Trial 21 finished with value: 0.041788102171099864 and parameters: {'learning_rate': 0.023163780519679288, 'num_leaves': 211, 'max_depth': 7, 'subsample': 0.8622417618661506, 'colsample_bytree': 0.7317980235855038}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:54,149] Trial 26 finished with value: 0.04231595438572873 and parameters: {'learning_rate': 0.06165606808569244, 'num_leaves': 141, 'max_depth': 11, 'subsample': 0.8199185701612776, 'colsample_bytree': 0.6285596070938871}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:54,339] Trial 23 finished with value: 0.04224379392195834 and parameters: {'learning_rate': 0.020641497782829428, 'num_leaves': 77, 'max_depth': 7, 'subsample': 0.8795890838542741, 'colsample_bytree': 0.7368004728673949}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:54,836] Trial 25 finished with value: 0.04114558706719227 and parameters: {'learning_rate': 0.09606715425545086, 'num_leaves': 79, 'max_depth': 11, 'subsample': 0.8225135529349172, 'colsample_bytree': 0.6090740051143053}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:55,358] Trial 24 finished with value: 0.04123008407686649 and parameters: {'learning_rate': 0.022718094748884244, 'num_leaves': 138, 'max_depth': 11, 'subsample': 0.8623169545647204, 'colsample_bytree': 0.6501732306315082}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:55,844] Trial 27 finished with value: 0.04110604809269352 and parameters: {'learning_rate': 0.09191513626863375, 'num_leaves': 78, 'max_depth': 11, 'subsample': 0.7548705484568276, 'colsample_bytree': 0.9904024471872825}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:56,130] Trial 28 finished with value: 0.041171220422268805 and parameters: {'learning_rate': 0.04403601767827308, 'num_leaves': 63, 'max_depth': 11, 'subsample': 0.7652327119273766, 'colsample_bytree': 0.6366983306307358}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:56,228] Trial 29 finished with value: 0.04110780402521415 and parameters: {'learning_rate': 0.07369256207739129, 'num_leaves': 59, 'max_depth': 9, 'subsample': 0.7629852957481893, 'colsample_bytree': 0.6350313482312739}. Best is trial 18 with value: 0.041015355057458125.\n",
      "[I 2025-07-19 01:35:56,357] A new study created in memory with name: no-name-152b444f-515f-4e2a-91a4-7a31f7594dbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 0.017062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best LGBM Params: {'learning_rate': 0.04641788831220958, 'num_leaves': 116, 'max_depth': 11, 'subsample': 0.7732171710988691, 'colsample_bytree': 0.8586909418857228}\n",
      "Final Test MAPE for LightGBM: 5.79%\n",
      "\n",
      "--- Training Model 3: CatBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-19 01:35:57,516] Trial 3 finished with value: 0.08271357714302481 and parameters: {'learning_rate': 0.04054944050561109, 'depth': 5, 'l2_leaf_reg': 5.735374599977004, 'random_strength': 2.8530750843592068, 'bagging_temperature': 0.08388515680403053, 'border_count': 35}. Best is trial 3 with value: 0.08271357714302481.\n",
      "[I 2025-07-19 01:35:57,744] Trial 1 finished with value: 0.35378029953720214 and parameters: {'learning_rate': 0.001674275903188653, 'depth': 5, 'l2_leaf_reg': 0.1610351780025505, 'random_strength': 0.0333521195376103, 'bagging_temperature': 0.14430083863730525, 'border_count': 56}. Best is trial 3 with value: 0.08271357714302481.\n",
      "[I 2025-07-19 01:35:57,986] Trial 0 finished with value: 0.035724022728896854 and parameters: {'learning_rate': 0.04557169074470192, 'depth': 5, 'l2_leaf_reg': 0.2939002547511921, 'random_strength': 3.551519236369881, 'bagging_temperature': 0.963918731855977, 'border_count': 181}. Best is trial 0 with value: 0.035724022728896854.\n",
      "[I 2025-07-19 01:35:59,374] Trial 4 finished with value: 0.036508558151126545 and parameters: {'learning_rate': 0.02749658289642738, 'depth': 6, 'l2_leaf_reg': 0.023178055235716612, 'random_strength': 0.0240216319739668, 'bagging_temperature': 0.5990240476823394, 'border_count': 127}. Best is trial 0 with value: 0.035724022728896854.\n",
      "[I 2025-07-19 01:36:00,835] Trial 2 finished with value: 0.024209650828588548 and parameters: {'learning_rate': 0.03280530896075551, 'depth': 9, 'l2_leaf_reg': 0.040110606425091344, 'random_strength': 0.027033156990627062, 'bagging_temperature': 0.4253266531858192, 'border_count': 235}. Best is trial 2 with value: 0.024209650828588548.\n",
      "[I 2025-07-19 01:36:00,867] Trial 5 finished with value: 0.04072600868292817 and parameters: {'learning_rate': 0.008625323703636409, 'depth': 7, 'l2_leaf_reg': 6.3948671562747235, 'random_strength': 5.70537002571001, 'bagging_temperature': 0.5413147560364591, 'border_count': 248}. Best is trial 2 with value: 0.024209650828588548.\n",
      "[I 2025-07-19 01:36:01,485] Trial 6 finished with value: 0.03627053107084131 and parameters: {'learning_rate': 0.05344085276252609, 'depth': 8, 'l2_leaf_reg': 0.004443806058291303, 'random_strength': 0.7382675167706552, 'bagging_temperature': 0.42556406837821437, 'border_count': 148}. Best is trial 2 with value: 0.024209650828588548.\n",
      "[I 2025-07-19 01:36:02,375] Trial 7 finished with value: 0.035640714665000024 and parameters: {'learning_rate': 0.008868284742739341, 'depth': 7, 'l2_leaf_reg': 0.4338010601381254, 'random_strength': 1.6244868143284583, 'bagging_temperature': 0.34303302843025996, 'border_count': 247}. Best is trial 2 with value: 0.024209650828588548.\n",
      "[I 2025-07-19 01:36:03,282] Trial 11 finished with value: 0.023975986203649814 and parameters: {'learning_rate': 0.02062429279315531, 'depth': 4, 'l2_leaf_reg': 2.500894478311855, 'random_strength': 0.08020037666065981, 'bagging_temperature': 0.8119487022093321, 'border_count': 249}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:04,313] Trial 9 finished with value: 0.13556317855291744 and parameters: {'learning_rate': 0.003305239236912455, 'depth': 9, 'l2_leaf_reg': 7.934355903626473, 'random_strength': 0.07462875122184061, 'bagging_temperature': 0.08184186966161766, 'border_count': 92}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:05,213] Trial 10 finished with value: 0.03501196980465267 and parameters: {'learning_rate': 0.08757911934008612, 'depth': 8, 'l2_leaf_reg': 1.3505517243886729, 'random_strength': 0.5002566194684609, 'bagging_temperature': 0.1570626789130798, 'border_count': 200}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:05,547] Trial 13 finished with value: 0.03595193968977589 and parameters: {'learning_rate': 0.016045130033292967, 'depth': 4, 'l2_leaf_reg': 1.0822540191372272, 'random_strength': 0.0015079069592222195, 'bagging_temperature': 0.8813895344645237, 'border_count': 189}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:05,898] Trial 8 finished with value: 0.02421957527208222 and parameters: {'learning_rate': 0.020821464869409678, 'depth': 9, 'l2_leaf_reg': 7.062927527199999, 'random_strength': 0.027069617979167963, 'bagging_temperature': 0.2350914541104332, 'border_count': 226}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:07,803] Trial 12 finished with value: 0.1510377797003825 and parameters: {'learning_rate': 0.0034627458277379265, 'depth': 9, 'l2_leaf_reg': 0.007590284078703481, 'random_strength': 0.003040539131929525, 'bagging_temperature': 0.2149520460726697, 'border_count': 113}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:10,711] Trial 15 finished with value: 0.024082394120141078 and parameters: {'learning_rate': 0.018254235174037545, 'depth': 9, 'l2_leaf_reg': 0.031279987164314765, 'random_strength': 0.005073654741360366, 'bagging_temperature': 0.7426401992902585, 'border_count': 219}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:18,530] Trial 14 finished with value: 0.03428960828108633 and parameters: {'learning_rate': 0.018501533927224718, 'depth': 10, 'l2_leaf_reg': 0.02900479350176169, 'random_strength': 0.002447653598458509, 'bagging_temperature': 0.7758867485703331, 'border_count': 211}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:19,188] Trial 16 finished with value: 0.05528150481764129 and parameters: {'learning_rate': 0.006226835157464196, 'depth': 10, 'l2_leaf_reg': 0.03394748442098567, 'random_strength': 0.0054233168012669496, 'bagging_temperature': 0.726024647398534, 'border_count': 217}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:19,291] Trial 19 finished with value: 0.09405412005067128 and parameters: {'learning_rate': 0.004796315511080358, 'depth': 4, 'l2_leaf_reg': 0.0011223182954098144, 'random_strength': 0.2175861073288214, 'bagging_temperature': 0.7084971052241757, 'border_count': 163}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:20,295] Trial 20 finished with value: 0.0366258820044885 and parameters: {'learning_rate': 0.01312721040505359, 'depth': 4, 'l2_leaf_reg': 0.0017017131898568686, 'random_strength': 0.18095261989579312, 'bagging_temperature': 0.6795648424011953, 'border_count': 177}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:20,708] Trial 17 finished with value: 0.027121304290845437 and parameters: {'learning_rate': 0.01406764750800015, 'depth': 10, 'l2_leaf_reg': 0.05292537446950433, 'random_strength': 0.23165302362576332, 'bagging_temperature': 0.7253383471389856, 'border_count': 217}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:21,074] Trial 22 finished with value: 0.02485173056646842 and parameters: {'learning_rate': 0.0862384171032225, 'depth': 6, 'l2_leaf_reg': 0.07799913988701428, 'random_strength': 0.006836571626770995, 'bagging_temperature': 0.870457793847887, 'border_count': 255}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:21,419] Trial 23 finished with value: 0.025246470549598575 and parameters: {'learning_rate': 0.09326514244428985, 'depth': 6, 'l2_leaf_reg': 0.8305004904818235, 'random_strength': 0.009411274492640112, 'bagging_temperature': 0.8659307090558608, 'border_count': 245}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:21,653] Trial 21 finished with value: 0.036205602665165995 and parameters: {'learning_rate': 0.012826767860033988, 'depth': 7, 'l2_leaf_reg': 0.09340812484586145, 'random_strength': 0.006692736967552792, 'bagging_temperature': 0.8687837066676245, 'border_count': 175}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:23,156] Trial 24 finished with value: 0.024121413144540173 and parameters: {'learning_rate': 0.029433118091055498, 'depth': 8, 'l2_leaf_reg': 0.010028221580520857, 'random_strength': 0.017147673296267686, 'bagging_temperature': 0.45922156507821965, 'border_count': 228}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:23,623] Trial 25 finished with value: 0.024311227064221872 and parameters: {'learning_rate': 0.02956937261223712, 'depth': 8, 'l2_leaf_reg': 0.014732398575739892, 'random_strength': 0.06535133225323253, 'bagging_temperature': 0.5838099100179733, 'border_count': 230}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:23,844] Trial 26 finished with value: 0.024269554513498673 and parameters: {'learning_rate': 0.030303455691186205, 'depth': 8, 'l2_leaf_reg': 0.009920369535025562, 'random_strength': 0.07810820569583347, 'bagging_temperature': 0.4797486583312099, 'border_count': 232}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:23,944] Trial 18 finished with value: 0.034497218559646506 and parameters: {'learning_rate': 0.01314324234000573, 'depth': 10, 'l2_leaf_reg': 0.0442958119114749, 'random_strength': 0.005289039807249219, 'bagging_temperature': 0.7393176971266309, 'border_count': 209}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:25,949] Trial 29 finished with value: 0.0353640795632556 and parameters: {'learning_rate': 0.021859503723885648, 'depth': 7, 'l2_leaf_reg': 0.004431016723094548, 'random_strength': 0.0010014559125239102, 'bagging_temperature': 0.9809517136124223, 'border_count': 203}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:26,297] Trial 27 finished with value: 0.035310583970321975 and parameters: {'learning_rate': 0.023700477159218726, 'depth': 8, 'l2_leaf_reg': 0.010193476570056354, 'random_strength': 0.0613689087163925, 'bagging_temperature': 0.5399684906939355, 'border_count': 199}. Best is trial 11 with value: 0.023975986203649814.\n",
      "[I 2025-07-19 01:36:26,697] Trial 28 finished with value: 0.03498806529619792 and parameters: {'learning_rate': 0.05932510421128311, 'depth': 8, 'l2_leaf_reg': 0.004545031502595058, 'random_strength': 0.01844688738577054, 'bagging_temperature': 0.4841796959651476, 'border_count': 202}. Best is trial 11 with value: 0.023975986203649814.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost Params: {'learning_rate': 0.02062429279315531, 'depth': 4, 'l2_leaf_reg': 2.500894478311855, 'random_strength': 0.08020037666065981, 'bagging_temperature': 0.8119487022093321, 'border_count': 249}\n",
      "Final Test MAPE for CatBoost: 2.67%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 5. MODEL COMPARISON ---\n",
    "\n",
    "# ## Model 1: Tuned Ridge Regression ##\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Training Model 1: Ridge Regression ---\")\n",
    "param_grid_ridge = {'alpha': [1.0, 10.0, 50.0, 100.0, 200.0, 500.0]}\n",
    "grid_search_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train, y_train.ravel())\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "y_pred_ridge = best_ridge.predict(X_test)\n",
    "y_pred_ridge_orig = pt.inverse_transform(y_pred_ridge.reshape(-1, 1))\n",
    "y_test_orig = pt.inverse_transform(y_test)\n",
    "mape_ridge = mean_absolute_percentage_error(y_test_orig, y_pred_ridge_orig) * 100\n",
    "print(f\"Best Ridge Alpha: {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Final Test MAPE for Ridge: {mape_ridge:.2f}%\")\n",
    "\n",
    "# ## Model 2: Tuned LightGBM (with Optuna) ##\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Training Model 2: LightGBM ---\")\n",
    "def objective_lgbm(trial):\n",
    "    params = { 'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 1000,\n",
    "               'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "               'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "               'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "               'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "               'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "               'random_state': 42, 'n_jobs': 4 }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train.ravel(), eval_set=[(X_test, y_test.ravel())], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    preds = model.predict(X_test)\n",
    "    return np.sqrt(np.mean((y_test - preds.reshape(-1,1))**2))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=30, n_jobs=4)\n",
    "best_params_lgbm = study_lgbm.best_params\n",
    "final_lgbm = lgb.LGBMRegressor(objective='regression_l1', random_state=42, n_jobs=-1, **best_params_lgbm)\n",
    "final_lgbm.fit(X_train, y_train.ravel())\n",
    "y_pred_lgbm = final_lgbm.predict(X_test)\n",
    "y_pred_lgbm_orig = pt.inverse_transform(y_pred_lgbm.reshape(-1, 1))\n",
    "mape_lgbm = mean_absolute_percentage_error(y_test_orig, y_pred_lgbm_orig) * 100\n",
    "print(f\"Best LGBM Params: {best_params_lgbm}\")\n",
    "print(f\"Final Test MAPE for LightGBM: {mape_lgbm:.2f}%\")\n",
    "\n",
    "# ## Model 3: Tuned CatBoost (with Optuna) ##\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Training Model 3: CatBoost ---\")\n",
    "def objective_catboost(trial):\n",
    "    params = { 'objective': 'MAE', 'eval_metric': 'RMSE', 'iterations': 1000,\n",
    "               'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "               'depth': trial.suggest_int('depth', 4, 10),\n",
    "               'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "               'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),\n",
    "               'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "               'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "               'random_state': 42, 'verbose': 0, 'thread_count': 4 }\n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train.ravel(), eval_set=[(X_test, y_test.ravel())], early_stopping_rounds=100, verbose=0)\n",
    "    preds = model.predict(X_test)\n",
    "    return np.sqrt(np.mean((y_test - preds.reshape(-1,1))**2))\n",
    "\n",
    "study_catboost = optuna.create_study(direction='minimize')\n",
    "study_catboost.optimize(objective_catboost, n_trials=30, n_jobs=4)\n",
    "best_params_catboost = study_catboost.best_params\n",
    "final_catboost = cb.CatBoostRegressor(objective='MAE', random_state=42, thread_count=-1, **best_params_catboost)\n",
    "final_catboost.fit(X_train, y_train.ravel(), verbose=0)\n",
    "y_pred_catboost = final_catboost.predict(X_test)\n",
    "y_pred_catboost_orig = pt.inverse_transform(y_pred_catboost.reshape(-1, 1))\n",
    "mape_catboost = mean_absolute_percentage_error(y_test_orig, y_pred_catboost_orig) * 100\n",
    "print(f\"Best CatBoost Params: {best_params_catboost}\")\n",
    "print(f\"Final Test MAPE for CatBoost: {mape_catboost:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d8bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "MODEL_SAVE_DIR = f\"saved_models_{TARGET_TO_SOLVE}\"\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a038840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving artifacts for BlendProperty4 ---\n",
      "Best model is CatBoost with MAPE: 2.67%\n",
      "All artifacts saved to saved_models_BlendProperty4\n"
     ]
    }
   ],
   "source": [
    "# --- 5. SAVE BEST MODEL AND ARTIFACTS ---\n",
    "print(f\"\\n--- Saving artifacts for {TARGET_TO_SOLVE} ---\")\n",
    "models = {'Ridge': (mape_ridge, best_ridge), \n",
    "          'LightGBM': (mape_lgbm, final_lgbm), \n",
    "          'CatBoost': (mape_catboost, final_catboost)}\n",
    "best_model_name = min(models, key=lambda k: models[k][0])\n",
    "best_model_obj = models[best_model_name][1]\n",
    "print(f\"Best model is {best_model_name} with MAPE: {models[best_model_name][0]:.2f}%\")\n",
    "\n",
    "# Save all necessary objects to reproduce the pipeline\n",
    "joblib.dump(feature_scaler, os.path.join(MODEL_SAVE_DIR, 'feature_scaler.joblib'))\n",
    "joblib.dump(pt, os.path.join(MODEL_SAVE_DIR, 'power_transformer.joblib'))\n",
    "joblib.dump(top_10_base_features, os.path.join(MODEL_SAVE_DIR, 'top_10_features.joblib'))\n",
    "joblib.dump(poly, os.path.join(MODEL_SAVE_DIR, 'polynomial_transformer.joblib'))\n",
    "joblib.dump(selector, os.path.join(MODEL_SAVE_DIR, 'rfecv_selector.joblib'))\n",
    "model_filename = os.path.join(MODEL_SAVE_DIR, 'best_model.joblib')\n",
    "if isinstance(best_model_obj, cb.CatBoostRegressor):\n",
    "    model_filename = os.path.join(MODEL_SAVE_DIR, 'best_model.cbm')\n",
    "    best_model_obj.save_model(model_filename)\n",
    "else:\n",
    "    joblib.dump(best_model_obj, model_filename)\n",
    "print(f\"All artifacts saved to {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce716697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. PREDICTION ON NEW DATA ---\n",
    "print(f\"\\n--- Generating predictions for {TARGET_TO_SOLVE} on {TEST_FILE} ---\")\n",
    "# Load test data\n",
    "test_data = pd.read_csv(\"train.csv\")\n",
    "if 'Id' not in test_data.columns:\n",
    "    test_data['Id'] = test_data.index\n",
    "\n",
    "# Load saved artifacts\n",
    "scaler = joblib.load(os.path.join(MODEL_SAVE_DIR, 'feature_scaler.joblib'))\n",
    "pt = joblib.load(os.path.join(MODEL_SAVE_DIR, 'power_transformer.joblib'))\n",
    "top_10 = joblib.load(os.path.join(MODEL_SAVE_DIR, 'top_10_features.joblib'))\n",
    "poly = joblib.load(os.path.join(MODEL_SAVE_DIR, 'polynomial_transformer.joblib'))\n",
    "selector = joblib.load(os.path.join(MODEL_SAVE_DIR, 'rfecv_selector.joblib'))\n",
    "try:\n",
    "    model = joblib.load(os.path.join(MODEL_SAVE_DIR, 'best_model.joblib'))\n",
    "except:\n",
    "    model = cb.CatBoostRegressor()\n",
    "    model.load_model(os.path.join(MODEL_SAVE_DIR, 'best_model.cbm'))\n",
    "\n",
    "# Apply the exact same transformation pipeline\n",
    "base_features_test = pd.concat([test_data[base_features_cols], pd.DataFrame(weighted_data), statistical_features], axis=1).copy()\n",
    "X_scaled_test = scaler.transform(base_features_test)\n",
    "X_scaled_df_test = pd.DataFrame(X_scaled_test, columns=base_features.columns)\n",
    "\n",
    "poly_features_test = poly.transform(X_scaled_df_test[top_10])\n",
    "poly_df_test = pd.DataFrame(poly_features_test, columns=poly.get_feature_names_out(top_10))\n",
    "X_advanced_test = pd.concat([X_scaled_df_test, poly_df_test], axis=1)\n",
    "X_advanced_test = X_advanced_test.loc[:,~X_advanced_test.columns.duplicated()].copy()\n",
    "\n",
    "X_final_test = selector.transform(X_advanced_test)\n",
    "\n",
    "# Predict and inverse transform\n",
    "predictions_transformed = model.predict(X_final_test)\n",
    "final_predictions = pt.inverse_transform(predictions_transformed.reshape(-1, 1))\n",
    "\n",
    "# Create final DataFrame\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Id': test_data['Id'],\n",
    "    TARGET_TO_SOLVE: final_predictions.flatten()\n",
    "})\n",
    "\n",
    "print(\"\\n--- Prediction DataFrame ---\")\n",
    "print(prediction_df.head())\n",
    "\n",
    "# You can save this DataFrame to a CSV\n",
    "# prediction_df.to_csv(f\"predictions_{TARGET_TO_SOLVE}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
